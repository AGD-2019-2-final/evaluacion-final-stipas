{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input/*': No such file or directory\n",
      "rm: `output/*': No such file or directory\n",
      "rm: `output2/*': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rm output2/*\n",
    "!hadoop fs -rmdir input output\n",
    "!hadoop fs -rmdir input output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `data.csv': File exists\n",
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup        615 2020-01-19 13:26 data.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put *.csv .\n",
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!mkdir input\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "-- fs -mkdir input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put input/ .\n",
    "\n",
    "-- carga de datos\n",
    "--Punto 30\n",
    "\n",
    "u = LOAD 'data.csv' USING PigStorage(',')\n",
    "    AS (f1:INT, f2:CHARARRAY, f3:CHARARRAY, f4:CHARARRAY, f5:CHARARRAY, f6: INT);\n",
    "\n",
    "    \n",
    "substringdata = FOREACH u GENERATE f4, SUBSTRING (f4, 8, 10), (SUBSTRING (f4, 8, 10) == '01' ? '1' : \n",
    "                         (SUBSTRING (f4, 8, 10) == '02' ? '2' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '03' ? '3' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '04' ? '4' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '05' ? '5' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '06' ? '6' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '07' ? '7' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '08' ? '8' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '09' ? '9' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '10' ? '10' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '11' ? '11' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '12' ? '12' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '13' ? '13' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '14' ? '14' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '15' ? '15' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '16' ? '16' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '17' ? '17' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '18' ? '18' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '19' ? '19' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '20' ? '20' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '21' ? '21' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '22' ? '22' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '23' ? '23' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '24' ? '24' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '25' ? '25' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '26' ? '26' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '27' ? '27' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '28' ? '28' :\n",
    "                         (SUBSTRING (f4, 8, 10) == '29' ? '29' : \n",
    "                         (SUBSTRING (f4, 8, 10) == '30' ? '30' : '31')))))))))))))))))))))))))))))), ToString( ToDate(f4,'yyyy-MM-dd'), 'EEE' ) ; \n",
    "\n",
    "cases = FOREACH substringdata GENERATE CONCAT($0, ',' , $1, ',' , $2, ',' , ($3 == 'Sun' ? 'dom' : \n",
    "                         ($3 == 'Mon' ? 'lun' :\n",
    "                         ($3 == 'Tue' ? 'mar' :\n",
    "                         ($3 == 'Wed' ? 'mie' :\n",
    "                         ($3 == 'Thu' ? 'jue' :\n",
    "                         ($3 == 'Fri' ? 'vie' : \n",
    "                         ($3 == 'Sat' ? 'sab' : 'sun'))))))), ',' , ($3 == 'Sun' ? 'domingo' : \n",
    "                         ($3 == 'Mon' ? 'lunes' :\n",
    "                         ($3 == 'Tue' ? 'martes' :\n",
    "                         ($3 == 'Wed' ? 'miercoles' :\n",
    "                         ($3 == 'Thu' ? 'jueves' :\n",
    "                         ($3 == 'Fri' ? 'viernes' :\n",
    "                         ($3 == 'Sat' ? 'sabado' : 'domingo'))))))));\n",
    "DUMP cases;\n",
    "\n",
    "\n",
    "-- escribe el archivo de salida\n",
    "STORE cases INTO 'output';\n",
    "\n",
    "-- copia los warchivos del HDFS al sistema local\n",
    "fs -get output/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 15:14:16,003 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2020-01-19 15:14:18,971 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:19,211 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2020-01-19 15:14:19,216 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2020-01-19 15:14:19,243 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2020-01-19 15:14:20,278 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2020-01-19 15:14:20,294 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:20,339 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2020-01-19 15:14:20,490 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-01-19 15:14:20,558 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-01-19 15:14:20,689 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-01-19 15:14:20,871 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1579440194973_0043\n",
      "2020-01-19 15:14:21,122 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-01-19 15:14:21,270 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1579440194973_0043\n",
      "2020-01-19 15:14:21,324 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://bde6d214a8ef:8088/proxy/application_1579440194973_0043/\n",
      "2020-01-19 15:14:41,510 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:41,523 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:41,760 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:41,767 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:41,817 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2020-01-19 15:14:41,821 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:41,827 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:41,953 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:41,963 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:42,024 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:42,030 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:42,088 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:42,095 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:14:42,195 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "(1971-07-08,08,8,jue,jueves)\n",
      "(1974-05-23,23,23,jue,jueves)\n",
      "(1973-04-22,22,22,dom,domingo)\n",
      "(1975-01-29,29,29,mie,miercoles)\n",
      "(1974-07-03,03,3,mie,miercoles)\n",
      "(1974-10-18,18,18,vie,viernes)\n",
      "(1970-10-05,05,5,lun,lunes)\n",
      "(1969-02-24,24,24,lun,lunes)\n",
      "(1974-10-17,17,17,jue,jueves)\n",
      "(1975-02-28,28,28,vie,viernes)\n",
      "(1969-12-07,07,7,dom,domingo)\n",
      "(1973-12-24,24,24,lun,lunes)\n",
      "(1970-08-27,27,27,jue,jueves)\n",
      "(1972-12-12,12,12,mar,martes)\n",
      "(1970-07-01,01,1,mie,miercoles)\n",
      "(1974-02-11,11,11,lun,lunes)\n",
      "(1973-04-01,01,1,dom,domingo)\n",
      "(1973-04-29,29,29,dom,domingo)\n",
      "2020-01-19 15:14:42,410 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2020-01-19 15:14:42,565 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:43,272 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:14:43,305 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-01-19 15:14:43,349 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-01-19 15:14:43,429 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-01-19 15:14:43,497 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1579440194973_0044\n",
      "2020-01-19 15:14:43,507 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-01-19 15:14:43,627 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1579440194973_0044\n",
      "2020-01-19 15:14:43,655 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://bde6d214a8ef:8088/proxy/application_1579440194973_0044/\n",
      "2020-01-19 15:15:03,917 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:03,928 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:15:04,006 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:04,013 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:15:04,057 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:04,065 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:15:04,121 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:04,126 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:15:04,186 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:04,193 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-19 15:15:04,240 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-19 15:15:04,254 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run script.pig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2020-01-19 15:15 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        509 2020-01-19 15:14 output/part-m-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1971-07-08,08,8,jue,jueves\n",
      "1974-05-23,23,23,jue,jueves\n",
      "1973-04-22,22,22,dom,domingo\n",
      "1975-01-29,29,29,mie,miercoles\n",
      "1974-07-03,03,3,mie,miercoles\n",
      "1974-10-18,18,18,vie,viernes\n",
      "1970-10-05,05,5,lun,lunes\n",
      "1969-02-24,24,24,lun,lunes\n",
      "1974-10-17,17,17,jue,jueves\n",
      "1975-02-28,28,28,vie,viernes\n",
      "1969-12-07,07,7,dom,domingo\n",
      "1973-12-24,24,24,lun,lunes\n",
      "1970-08-27,27,27,jue,jueves\n",
      "1972-12-12,12,12,mar,martes\n",
      "1970-07-01,01,1,mie,miercoles\n",
      "1974-02-11,11,11,lun,lunes\n",
      "1973-04-01,01,1,dom,domingo\n",
      "1973-04-29,29,29,dom,domingo\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input/*': No such file or directory\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-m-00000\n",
      "rm: `output2/*': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rm output2/*\n",
    "!hadoop fs -rmdir input output\n",
    "!hadoop fs -rmdir input output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input\n",
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
