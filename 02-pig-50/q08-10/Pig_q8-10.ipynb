{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pig_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input/*': No such file or directory\n",
      "rm: `output/*': No such file or directory\n",
      "rm: `output2/*': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rm output2/*\n",
    "!hadoop fs -rmdir input output\n",
    "!hadoop fs -rmdir input output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `data.tsv': File exists\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup       1741 2020-01-18 22:51 data.tsv\n",
      "-rw-r--r--   1 root supergroup    2271958 2020-01-18 22:32 truck_event_text_partition.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put *.tsv .\n",
    "!hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input\n",
    "!rm -rf output\n",
    "!mkdir input\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing script.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.pig\n",
    "\n",
    "-- crea la carpeta input in el HDFS\n",
    "-- fs -mkdir input\n",
    "\n",
    "-- copia los archivos del sistema local al HDFS\n",
    "fs -put input/ .\n",
    "\n",
    "-- carga de datos\n",
    "--Punto 8\n",
    "u = LOAD 'data.tsv'\n",
    "    AS (f1:CHARARRAY, f2:BAG{}, f3:MAP[]);\n",
    "\n",
    "r = FOREACH u GENERATE FLATTEN(f2), FLATTEN(f3);\n",
    "\n",
    "y = GROUP r BY ($0, $1);\n",
    "\n",
    "z = FOREACH y GENERATE group,  COUNT(r);\n",
    "\n",
    "DUMP z;\n",
    "\n",
    "-- escribe el archivo de salida\n",
    "STORE z INTO 'output';\n",
    "\n",
    "-- copia los archivos del HDFS al sistema local\n",
    "fs -get output/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 23:17:20,483 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2020-01-18 23:17:23,118 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:23,427 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2020-01-18 23:17:23,433 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2020-01-18 23:17:23,453 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2020-01-18 23:17:23,470 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\n",
      "2020-01-18 23:17:24,025 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker.http.address is deprecated. Instead, use mapreduce.jobtracker.http.address\n",
      "2020-01-18 23:17:24,050 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:24,094 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2020-01-18 23:17:24,254 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-01-18 23:17:24,300 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-01-18 23:17:24,429 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-01-18 23:17:24,616 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1579380588467_0088\n",
      "2020-01-18 23:17:24,862 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-01-18 23:17:24,951 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1579380588467_0088\n",
      "2020-01-18 23:17:24,996 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://58c11497d4fd:8088/proxy/application_1579380588467_0088/\n",
      "2020-01-18 23:17:50,187 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,197 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,395 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,403 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,466 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,473 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,589 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,595 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,646 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,654 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,707 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:50,712 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:17:50,820 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "((a,aaa),5)\n",
      "((a,bbb),7)\n",
      "((a,ccc),13)\n",
      "((a,ddd),13)\n",
      "((a,eee),7)\n",
      "((a,fff),10)\n",
      "((a,ggg),8)\n",
      "((a,hhh),8)\n",
      "((a,iii),7)\n",
      "((a,jjj),10)\n",
      "((b,aaa),4)\n",
      "((b,bbb),7)\n",
      "((b,ccc),5)\n",
      "((b,ddd),7)\n",
      "((b,eee),5)\n",
      "((b,fff),8)\n",
      "((b,ggg),4)\n",
      "((b,hhh),7)\n",
      "((b,iii),7)\n",
      "((b,jjj),5)\n",
      "((c,aaa),5)\n",
      "((c,bbb),4)\n",
      "((c,ccc),12)\n",
      "((c,ddd),9)\n",
      "((c,eee),6)\n",
      "((c,fff),8)\n",
      "((c,ggg),7)\n",
      "((c,hhh),7)\n",
      "((c,iii),8)\n",
      "((c,jjj),8)\n",
      "((d,aaa),4)\n",
      "((d,bbb),6)\n",
      "((d,ccc),7)\n",
      "((d,ddd),5)\n",
      "((d,eee),6)\n",
      "((d,fff),8)\n",
      "((d,ggg),6)\n",
      "((d,hhh),4)\n",
      "((d,iii),9)\n",
      "((d,jjj),8)\n",
      "((e,aaa),3)\n",
      "((e,bbb),8)\n",
      "((e,ccc),9)\n",
      "((e,ddd),7)\n",
      "((e,eee),7)\n",
      "((e,fff),9)\n",
      "((e,ggg),4)\n",
      "((e,hhh),4)\n",
      "((e,iii),8)\n",
      "((e,jjj),3)\n",
      "((f,aaa),8)\n",
      "((f,bbb),10)\n",
      "((f,ccc),13)\n",
      "((f,ddd),17)\n",
      "((f,eee),11)\n",
      "((f,fff),11)\n",
      "((f,ggg),9)\n",
      "((f,hhh),10)\n",
      "((f,iii),10)\n",
      "((f,jjj),12)\n",
      "((g,aaa),3)\n",
      "((g,bbb),3)\n",
      "((g,ccc),6)\n",
      "((g,ddd),5)\n",
      "((g,eee),4)\n",
      "((g,fff),5)\n",
      "((g,ggg),4)\n",
      "((g,hhh),3)\n",
      "((g,iii),4)\n",
      "((g,jjj),6)\n",
      "2020-01-18 23:17:50,993 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2020-01-18 23:17:51,094 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:51,684 [JobControl] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:17:51,736 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2020-01-18 23:17:51,755 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2020-01-18 23:17:51,809 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2020-01-18 23:17:51,865 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1579380588467_0089\n",
      "2020-01-18 23:17:51,870 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\n",
      "2020-01-18 23:17:51,971 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1579380588467_0089\n",
      "2020-01-18 23:17:51,976 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://58c11497d4fd:8088/proxy/application_1579380588467_0089/\n",
      "2020-01-18 23:18:17,273 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,281 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:18:17,364 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,371 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:18:17,419 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,429 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:18:17,487 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,491 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:18:17,540 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,545 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n",
      "2020-01-18 23:18:17,596 [main] INFO  org.apache.hadoop.yarn.client.RMProxy - Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-01-18 23:18:17,607 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\n"
     ]
    }
   ],
   "source": [
    "!pig -execute 'run script.pig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2020-01-18 23:18 output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup        713 2020-01-18 23:18 output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a,aaa)\t5\n",
      "(a,bbb)\t7\n",
      "(a,ccc)\t13\n",
      "(a,ddd)\t13\n",
      "(a,eee)\t7\n",
      "(a,fff)\t10\n",
      "(a,ggg)\t8\n",
      "(a,hhh)\t8\n",
      "(a,iii)\t7\n",
      "(a,jjj)\t10\n",
      "(b,aaa)\t4\n",
      "(b,bbb)\t7\n",
      "(b,ccc)\t5\n",
      "(b,ddd)\t7\n",
      "(b,eee)\t5\n",
      "(b,fff)\t8\n",
      "(b,ggg)\t4\n",
      "(b,hhh)\t7\n",
      "(b,iii)\t7\n",
      "(b,jjj)\t5\n",
      "(c,aaa)\t5\n",
      "(c,bbb)\t4\n",
      "(c,ccc)\t12\n",
      "(c,ddd)\t9\n",
      "(c,eee)\t6\n",
      "(c,fff)\t8\n",
      "(c,ggg)\t7\n",
      "(c,hhh)\t7\n",
      "(c,iii)\t8\n",
      "(c,jjj)\t8\n",
      "(d,aaa)\t4\n",
      "(d,bbb)\t6\n",
      "(d,ccc)\t7\n",
      "(d,ddd)\t5\n",
      "(d,eee)\t6\n",
      "(d,fff)\t8\n",
      "(d,ggg)\t6\n",
      "(d,hhh)\t4\n",
      "(d,iii)\t9\n",
      "(d,jjj)\t8\n",
      "(e,aaa)\t3\n",
      "(e,bbb)\t8\n",
      "(e,ccc)\t9\n",
      "(e,ddd)\t7\n",
      "(e,eee)\t7\n",
      "(e,fff)\t9\n",
      "(e,ggg)\t4\n",
      "(e,hhh)\t4\n",
      "(e,iii)\t8\n",
      "(e,jjj)\t3\n",
      "(f,aaa)\t8\n",
      "(f,bbb)\t10\n",
      "(f,ccc)\t13\n",
      "(f,ddd)\t17\n",
      "(f,eee)\t11\n",
      "(f,fff)\t11\n",
      "(f,ggg)\t9\n",
      "(f,hhh)\t10\n",
      "(f,iii)\t10\n",
      "(f,jjj)\t12\n",
      "(g,aaa)\t3\n",
      "(g,bbb)\t3\n",
      "(g,ccc)\t6\n",
      "(g,ddd)\t5\n",
      "(g,eee)\t4\n",
      "(g,fff)\t5\n",
      "(g,ggg)\t4\n",
      "(g,hhh)\t3\n",
      "(g,iii)\t4\n",
      "(g,jjj)\t6\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input/*': No such file or directory\n",
      "Deleted output/_SUCCESS\n",
      "Deleted output/part-r-00000\n",
      "rm: `output2/*': No such file or directory\n",
      "rmdir: `input': No such file or directory\n",
      "rmdir: `output2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "## Se elimina el directorio de salida en el hdfs si existe\n",
    "!hadoop fs -rm input/*\n",
    "!hadoop fs -rm output/*\n",
    "!hadoop fs -rm output2/*\n",
    "!hadoop fs -rmdir input output\n",
    "!hadoop fs -rmdir input output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input\n",
    "!rm -rf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
